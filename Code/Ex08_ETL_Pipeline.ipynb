{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 8: Extracting and Transforming Data**\n",
    "\n",
    "In this tutorial, we will walk through the process of **extracting** and **transforming** data from a SQL database using Python. We will:\n",
    "- Connect to a SQL Server database using `SQLAlchemy` and `pyodbc`\n",
    "- Extract data from the **Sales.Orders** table\n",
    "- Transform the data to filter only **date-related columns**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Install Required Libraries**\n",
    "Before starting, ensure you have the required Python libraries installed. If not, install them using:\n",
    "\n",
    "```bash\n",
    "pip install pandas sqlalchemy pyodbc\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Import Required Libraries**\n",
    "We begin by importing the necessary libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Define the Database Connection**\n",
    "We use **SQLAlchemy** to create a connection string for a Microsoft SQL Server database.\n",
    "\n",
    "```python\n",
    "# Define the ODBC connection parameters\n",
    "driver = 'ODBC Driver 18 for SQL Server' \n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{{driver}}};SERVER={server};DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password};ENCRYPT=yes;TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "```\n",
    "\n",
    "Replace:\n",
    "- **`your_server`** with the actual SQL Server name.\n",
    "- **`your_database`** with your database name.\n",
    "- **`your_username`** and **`your_password`** with your credentials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "server = os.getenv(\"DB_SERVER\")\n",
    "database = os.getenv(\"DB_NAME\")\n",
    "username = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\") \n",
    "driver = 'ODBC Driver 18 for SQL Server' \n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{{driver}}};SERVER={server};DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password};ENCRYPT=yes;TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "query = \"\"\" \n",
    "    SELECT * FROM Sales.Orders;\n",
    "    \"\"\"\n",
    "\n",
    "raw_data = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Extract Data from SQL Database**\n",
    "### **Explanation:**\n",
    "- The `extract()` function executes an SQL query to retrieve all columns from the **Sales.Orders** table.\n",
    "- `pd.read_sql()` fetches the query results into a **pandas DataFrame**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract(engine):\n",
    "    query = \"\"\" \n",
    "        SELECT * FROM Sales.Orders;\n",
    "    \"\"\"\n",
    "    raw_data = pd.read_sql(query, engine)\n",
    "\n",
    "    return raw_data\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "raw_sales_data  = extract(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Transform the Data**\n",
    "Define a function to filter the data, keeping only columns that contain **date-related** information.\n",
    "\n",
    "```python\n",
    "Example codes: \n",
    "# Extract Columns\n",
    "no_null_columns = raw_data.columns[~raw_data.isnull().any()].to_list()\n",
    "clean_data = raw_data[no_null_columns]\n",
    "# record filtering\n",
    "start_date = pd.to_datetime('2010-01-01').date()\n",
    "clean_data = raw_data.loc[raw_data['OrderDate']>start_date, :]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the transform function to return the columns that includes the date data. \n",
    "def transform(raw_data):\n",
    "  \t# Edit the below codes to Filter rows and columns\n",
    "    clean_data = raw_data.copy()\n",
    "    return clean_data\n",
    "\n",
    "clean_data = transform(raw_sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Expected Output**\n",
    "If the `Sales.Orders` table has columns like:\n",
    "| OrderID | OrderDate   | ExpectedDeliveryDate  | PickingCompletedWhen | LastEditedWhen |\n",
    "|---------|------------|-----------|------------|-------------|\n",
    "| 1       | 2025-01-01 | 2025-01-03 | C001       | 150.00      |\n",
    "| 2       | 2025-02-15 | 2025-02-17 | C002       | 200.00      |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OrderID  CustomerID  SalespersonPersonID  PickedByPersonID  \\\n",
      "0        1         832                    2               NaN   \n",
      "1        2         803                    8               NaN   \n",
      "2        3         105                    7               NaN   \n",
      "3        4          57                   16               3.0   \n",
      "4        5         905                    3               NaN   \n",
      "\n",
      "   ContactPersonID  BackorderOrderID   OrderDate ExpectedDeliveryDate  \\\n",
      "0             3032              45.0  2013-01-01           2013-01-02   \n",
      "1             3003              46.0  2013-01-01           2013-01-02   \n",
      "2             1209              47.0  2013-01-01           2013-01-02   \n",
      "3             1113               NaN  2013-01-01           2013-01-02   \n",
      "4             3105              48.0  2013-01-01           2013-01-02   \n",
      "\n",
      "  CustomerPurchaseOrderNumber  IsUndersupplyBackordered Comments  \\\n",
      "0                       12126                      True     None   \n",
      "1                       15342                      True     None   \n",
      "2                       12211                      True     None   \n",
      "3                       17129                      True     None   \n",
      "4                       10369                      True     None   \n",
      "\n",
      "  DeliveryInstructions InternalComments PickingCompletedWhen  LastEditedBy  \\\n",
      "0                 None             None  2013-01-01 12:00:00             7   \n",
      "1                 None             None  2013-01-01 12:00:00             7   \n",
      "2                 None             None  2013-01-01 12:00:00             7   \n",
      "3                 None             None  2013-01-01 11:00:00             3   \n",
      "4                 None             None  2013-01-01 12:00:00             7   \n",
      "\n",
      "       LastEditedWhen  \n",
      "0 2013-01-01 12:00:00  \n",
      "1 2013-01-01 12:00:00  \n",
      "2 2013-01-01 12:00:00  \n",
      "3 2013-01-01 11:00:00  \n",
      "4 2013-01-01 12:00:00  \n"
     ]
    }
   ],
   "source": [
    "# Extract raw data from the database\n",
    "raw_sales_data = extract(engine)\n",
    "\n",
    "# Transform the raw data to include only date/time columns\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Display the transformed data\n",
    "print(clean_sales_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Load the Data**\n",
    "After transforming the data, save it to a CSV file for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to clean_sales_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the output file path\n",
    "output_path = \"clean_sales_data.csv\"\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "clean_sales_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache **Parquet** is a columnar storage format that is optimized for big data processing. Unlike traditional row-based formats (such as CSV), Parquet stores data by columns, making it more efficient for analytical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to clean_sales_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Define the output file path\n",
    "output_path = \"clean_sales_data.parquet\"\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "clean_sales_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
