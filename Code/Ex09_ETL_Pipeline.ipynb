{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tutorial 9: Extracting and Transforming Data**\n",
    "\n",
    "In this tutorial, we will walk through the process of **extracting** and **transforming** data from a SQL database using Python. We will:\n",
    "- Connect to a SQL Server database using `SQLAlchemy` and `pyodbc`\n",
    "- Extract data from the **Sales.Orders** table\n",
    "- Transform the data to filter only **date-related columns**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Install Required Libraries**\n",
    "Before starting, ensure you have the required Python libraries installed. If not, install them using:\n",
    "\n",
    "```bash\n",
    "pip install pandas sqlalchemy pyodbc\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Import Required Libraries**\n",
    "We begin by importing the necessary libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Define the Database Connection**\n",
    "We use **SQLAlchemy** to create a connection string for a Microsoft SQL Server database.\n",
    "\n",
    "```python\n",
    "# Define the ODBC connection parameters\n",
    "driver = 'ODBC Driver 18 for SQL Server' \n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{{driver}}};SERVER={server};DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password};ENCRYPT=yes;TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "```\n",
    "\n",
    "Replace:\n",
    "- **`your_server`** with the actual SQL Server name.\n",
    "- **`your_database`** with your database name.\n",
    "- **`your_username`** and **`your_password`** with your credentials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "server = os.getenv(\"DB_SERVER\")\n",
    "database = os.getenv(\"DB_NAME\")\n",
    "username = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\") \n",
    "driver = 'ODBC Driver 18 for SQL Server' \n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{{driver}}};SERVER={server};DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password};ENCRYPT=yes;TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "query = \"\"\" \n",
    "    SELECT * FROM Sales.Orders;\n",
    "    \"\"\"\n",
    "\n",
    "raw_data = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Extract Data from SQL Database**\n",
    "### **Explanation:**\n",
    "- The `extract()` function executes an SQL query to retrieve all columns from the **Sales.Orders** table.\n",
    "- `pd.read_sql()` fetches the query results into a **pandas DataFrame**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract(engine):\n",
    "    query = \"\"\" \n",
    "        SELECT * FROM Sales.Orders;\n",
    "    \"\"\"\n",
    "    raw_data = pd.read_sql(query, engine)\n",
    "\n",
    "    return raw_data\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "raw_sales_data  = extract(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Transform the Data**\n",
    "Define a function that filters the dataset by keeping only columns that **do not contain null values**. Ensure that **date-related columns** are properly formatted as **datetime** objects.\n",
    "\n",
    "```python\n",
    "Example codes: \n",
    "# Extract Columns that do not have null values\n",
    "no_null_columns = raw_data.columns[~raw_data.isnull().any()].to_list()\n",
    "clean_data = raw_data[no_null_columns]\n",
    "# Convert date-related columns to datetime if applicable\n",
    "clean_data[col] = pd.to_datetime(clean_data[col])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the transform function to return the columns that includes the date data. \n",
    "def transform(raw_data):\n",
    "  \t# Edit the below codes to Filter rows and columns\n",
    "    clean_data = raw_data.copy()\n",
    "    \n",
    "    return clean_data\n",
    "\n",
    "clean_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Expected Output**\n",
    "\n",
    "| OrderID | CustomerID | SalespersonPersonID | ContactPersonID | OrderDate  | ExpectedDeliveryDate | CustomerPurchaseOrderNumber | IsUndersupplyBackordered | LastEditedBy | LastEditedWhen         |\n",
    "|---------|-----------|---------------------|-----------------|------------|----------------------|----------------------------|--------------------------|--------------|------------------------|\n",
    "| 1       | 832       | 2                   | 3032            | 2013-01-01 | 2013-01-02          | 12126                      | True                     | 7            | 2013-01-01 12:00:00    |\n",
    "| 2       | 803       | 8                   | 3003            | 2013-01-01 | 2013-01-02          | 15342                      | True                     | 7            | 2013-01-01 12:00:00    |\n",
    "| 3       | 105       | 7                   | 1209            | 2013-01-01 | 2013-01-02          | 12211                      | True                     | 7            | 2013-01-01 12:00:00    |\n",
    "| 4       | 57        | 16                  | 1113            | 2013-01-01 | 2013-01-02          | 17129                      | True                     | 3            | 2013-01-01 11:00:00    |\n",
    "| 5       | 905       | 3                   | 3105            | 2013-01-01 | 2013-01-02          | 10369                      | True                     | 7            | 2013-01-01 12:00:00    |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw data from the database\n",
    "raw_sales_data = extract(engine)\n",
    "\n",
    "# Transform the raw data to include only date/time columns\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Display the transformed data\n",
    "display(clean_sales_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Load the Data**\n",
    "After transforming the data, save it to a CSV file for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_path = \"./data/processed/clean_sales_data.csv\"\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "clean_sales_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache **Parquet** is a columnar storage format that is optimized for big data processing. Unlike traditional row-based formats (such as CSV), Parquet stores data by columns, making it more efficient for analytical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_path = \"./data/processed/clean_sales_data.parquet\"\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "clean_sales_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
